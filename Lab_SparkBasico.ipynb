{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.espol.edu.ec/sites/default/files/nuevaespol/logo_negro.png\" width=\"301\" height=\"58\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#     FACULTAD DE INGENIER&Iacute;A EN ELECTRICIDAD Y COMPUTACI&Oacute;N\n",
    "##   DIPLOMADO EN CIENCIA DE DATOS PARA LOS NEGOCIOS\n",
    "###  Laboratorio 5: Docker & Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><strong>Introducci&oacute;n</strong></h3>\n",
    "\n",
    "Docker es un proyecto de código abierto que automatiza el despliegue de aplicaciones dentro de contenedores de software, proporcionando una capa adicional de abstracción y automatización de virtualización de aplicaciones en múltiples sistemas operativos.\n",
    "\n",
    "Apache Spark es un framework de computación en clúster open-source. Fue desarrollada originariamente en la Universidad de California, en el AMPLab de Berkeley. El código base del proyecto Spark fue donado más tarde a la Apache Software Foundation que se encarga de su mantenimiento desde entonces. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Procedimiento</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Paso 1: Instalación de contenedores</h3>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Instalación de Jupyter\n",
    "\n",
    "docker run -ti -p 8888:8888 emanuelfontelles/spark-jupyter"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Instalación de pyspark dentro del contenedor\n",
    "\n",
    "pip install pyspark #Trabajos con Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Paso 2: Comprobación de pyspark</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un RDD es una estructura de datos paralela que distribuye la carga de trabajo entre los nodos de trabajo. Son las unidades básicas de programación de Spark.\n",
    "Para trabajar con RDD, necesitamos crear un SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un SparkContext es la puerta de entrada para el entorno Spark. Para cada SparkApp necesita crear el objeto SparkContext. Permite que su aplicación Spark acceda a Spark Cluster con la ayuda de Resource Manager (Spark Standalone / Yarn / Mesos). Para crear SparkContext, se debe hacer el primer SparkConf. SparkConf tiene un parámetro de configuración que nuestra aplicación de controlador de Spark pasará a SparkContext.\n",
    "\n",
    "En Spark 2 puede usar SparkSession en lugar de SparkContext. En Spark 2.0 en adelante, es mejor usar SparkSession ya que proporciona acceso a todas las funcionalidades de Spark que proporciona SparkContext. Además, proporciona API para trabajar con DataFrames y DataSets.\n",
    "\n",
    "El siguiente código es útil cuando desea crear una SparkSession;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.master(\"local[*]\").appName(\"WordCount\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si escribimos local [*] en el maestro, usará todos los núcleos en nuestra máquina. Si dijimos local [4] funcionará con 4 núcleos. Puede funcionar en modo local / hilo / mesos y kubernetes.\n",
    "\n",
    "getOrCreate se usa para verificar si existe una SparkSession. Crea uno nuevo si no.\n",
    "\n",
    "Si no importamos SparkConf, tendremos que asignar métricas complejas para principiantes, como los valores \"executeor.memory\" y \"driver.memory\". En tales casos, puede determinar los valores de configuración como en el siguiente ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos una archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Project Gutenberg EBook of Romeo and Juliet, by William Shakespeare',\n",
       " '',\n",
       " '',\n",
       " '*******************************************************************',\n",
       " \"THIS EBOOK WAS ONE OF PROJECT GUTENBERG'S EARLY FILES PRODUCED AT A\",\n",
       " 'TIME WHEN PROOFING METHODS AND TOOLS WERE NOT WELL DEVELOPED. THERE',\n",
       " 'IS AN IMPROVED EDITION OF THIS TITLE WHICH MAY BE VIEWED AS EBOOK',\n",
       " '(#100) at https://www.gutenberg.org/ebooks/100',\n",
       " '*******************************************************************',\n",
       " '',\n",
       " '',\n",
       " 'This eBook is for the use of anyone anywhere at no cost and with',\n",
       " 'almost no restrictions whatsoever.  You may copy it, give it away or',\n",
       " 're-use it under the terms of the Project Gutenberg License included',\n",
       " 'with this eBook or online at www.gutenberg.org/license',\n",
       " '',\n",
       " '',\n",
       " 'Title: Romeo and Juliet',\n",
       " '',\n",
       " 'Author: William Shakespeare',\n",
       " '',\n",
       " 'Posting Date: May 25, 2012 [EBook #1112]',\n",
       " 'Release Date: November, 1997  [Etext #1112]',\n",
       " '',\n",
       " 'Language: English',\n",
       " '',\n",
       " 'Character set encoding: ASCII',\n",
       " '',\n",
       " '*** START OF THIS PROJECT GUTENBERG EBOOK ROMEO AND JULIET ***',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '*Project Gutenberg is proud to cooperate with The World Library*',\n",
       " 'in the presentation of The Complete Works of William Shakespeare',\n",
       " 'for your reading for education and entertainment.  HOWEVER, THIS',\n",
       " 'IS NEITHER SHAREWARE NOR PUBLIC DOMAIN. . .AND UNDER THE LIBRARY',\n",
       " 'OF THE FUTURE CONDITIONS OF THIS PRESENTATION. . .NO CHARGES MAY',\n",
       " 'BE MADE FOR *ANY* ACCESS TO THIS MATERIAL.  YOU ARE ENCOURAGED!!',\n",
       " 'TO GIVE IT AWAY TO ANYONE YOU LIKE, BUT NO CHARGES ARE ALLOWED!!',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'The Complete Works of William Shakespeare',\n",
       " '',\n",
       " 'The Tragedy of Romeo and Juliet',\n",
       " '',\n",
       " 'The Library of the Future Complete Works of William Shakespeare',\n",
       " 'Library of the Future is a TradeMark (TM) of World Library Inc.',\n",
       " '',\n",
       " '',\n",
       " '<<THIS ELECTRONIC VERSION OF THE COMPLETE WORKS OF WILLIAM',\n",
       " 'SHAKESPEARE IS COPYRIGHT 1990-1993 BY WORLD LIBRARY, INC., AND IS',\n",
       " 'PROVIDED BY PROJECT GUTENBERG ETEXT OF CARNEGIE MELLON UNIVERSITY',\n",
       " 'WITH PERMISSION.  ELECTRONIC AND MACHINE READABLE COPIES MAY BE',\n",
       " 'DISTRIBUTED SO LONG AS SUCH COPIES (1) ARE FOR YOUR OR OTHERS',\n",
       " 'PERSONAL USE ONLY, AND (2) ARE NOT DISTRIBUTED OR USED',\n",
       " 'COMMERCIALLY.  PROHIBITED COMMERCIAL DISTRIBUTION INCLUDES BY ANY',\n",
       " 'SERVICE THAT CHARGES FOR DOWNLOAD TIME OR FOR MEMBERSHIP.>>',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '1595',\n",
       " '',\n",
       " 'THE TRAGEDY OF ROMEO AND JULIET',\n",
       " '',\n",
       " 'by William Shakespeare',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Dramatis Personae',\n",
       " '',\n",
       " '  Chorus.',\n",
       " '',\n",
       " '',\n",
       " '  Escalus, Prince of Verona.',\n",
       " '',\n",
       " '  Paris, a young Count, kinsman to the Prince.',\n",
       " '',\n",
       " '  Montague, heads of two houses at variance with each other.',\n",
       " '',\n",
       " '  Capulet, heads of two houses at variance with each other.',\n",
       " '',\n",
       " '  An old Man, of the Capulet family.',\n",
       " '',\n",
       " '  Romeo, son to Montague.',\n",
       " '',\n",
       " '  Tybalt, nephew to Lady Capulet.',\n",
       " '']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location=\"/home/user/notebooks/data.txt\"\n",
    "data_rdd=sc.textFile(location)\n",
    "data_rdd.take(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Eliminar puntuación y transformar todas las palabras en minúsculas</strong>\n",
    "\n",
    "Para excluir los valores de puntuación y convertir todas las palabras en minúsculas, escribimos una función como la siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the project gutenberg ebook of romeo and juliet by william shakespeare',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'this ebook was one of project gutenbergs early files produced at a',\n",
       " 'time when proofing methods and tools were not well developed there',\n",
       " 'is an improved edition of this title which may be viewed as ebook',\n",
       " '100 at httpswwwgutenbergorgebooks100',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'this ebook is for the use of anyone anywhere at no cost and with',\n",
       " 'almost no restrictions whatsoever  you may copy it give it away or',\n",
       " 'reuse it under the terms of the project gutenberg license included',\n",
       " 'with this ebook or online at wwwgutenbergorglicense',\n",
       " '',\n",
       " '',\n",
       " 'title romeo and juliet',\n",
       " '',\n",
       " 'author william shakespeare',\n",
       " '',\n",
       " 'posting date may 25 2012 ebook 1112',\n",
       " 'release date november 1997  etext 1112',\n",
       " '',\n",
       " 'language english',\n",
       " '',\n",
       " 'character set encoding ascii',\n",
       " '',\n",
       " ' start of this project gutenberg ebook romeo and juliet ',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'project gutenberg is proud to cooperate with the world library',\n",
       " 'in the presentation of the complete works of william shakespeare',\n",
       " 'for your reading for education and entertainment  however this',\n",
       " 'is neither shareware nor public domain  and under the library',\n",
       " 'of the future conditions of this presentation  no charges may',\n",
       " 'be made for any access to this material  you are encouraged!!',\n",
       " 'to give it away to anyone you like but no charges are allowed!!',\n",
       " '']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lower_clean_str(x):\n",
    "    punc=':\"#$%&\\'()*+,./:;<=>?%[\\\\]^_`{|}~-'\n",
    "    lowercased_str = x.lower()\n",
    "    for ch in punc:\n",
    "        lowercased_str = lowercased_str.replace(ch, '')\n",
    "    return lowercased_str\n",
    "\n",
    "data_rdd=data_rdd.map(lower_clean_str)\n",
    "data_rdd.take(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Divide la oración en una lista de palabras</strong>\n",
    "\n",
    "La función split () divide la cadena en el parámetro dado y estamos dividiéndonos por el carácter de espacio.\n",
    "Usamos la función de división para separar las palabras en todas las líneas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'project', 'gutenberg', 'ebook', 'of']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rdd=data_rdd.flatMap(lambda satir: satir.split(\" \"))\n",
    "data_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Excluir espacios en blanco</strong>\n",
    "\n",
    "Realizamos un filtrado a continuación para excluir espacios en blanco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'project',\n",
       " 'gutenberg',\n",
       " 'ebook',\n",
       " 'of',\n",
       " 'romeo',\n",
       " 'and',\n",
       " 'juliet',\n",
       " 'by',\n",
       " 'william']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rdd=data_rdd.filter(lambda x:x!='')\n",
    "data_rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Cuenta cuántas veces aparece cada palabra</strong>\n",
    "\n",
    "Para hacer este cálculo, podemos aplicar la transformación \"reduceByKey\" en el par (clave, val) RDD.\n",
    "\n",
    "Para usar la transformación \"reduceByKey\" para encontrar las frecuencias de cada palabra, en primer lugar, se requiere un par (clave, val) RDD; En este par (clave, val) RDD, clave es la palabra y val es 1 para cada palabra en RDD (1 representa el número de cada palabra en \"data_rdd\").\n",
    "\n",
    "Al principio, necesitamos convertir \"data_rdd\" en par (clave, val) RDD (data_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1),\n",
       " ('project', 1),\n",
       " ('gutenberg', 1),\n",
       " ('ebook', 1),\n",
       " ('of', 1),\n",
       " ('romeo', 1),\n",
       " ('and', 1),\n",
       " ('juliet', 1),\n",
       " ('by', 1),\n",
       " ('william', 1)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_count=data_rdd.map(lambda word:(word,1))\n",
    "data_count.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 16),\n",
       " ('100', 1),\n",
       " ('1112', 2),\n",
       " ('1112txt', 1),\n",
       " ('1112zip', 1),\n",
       " ('1500', 1),\n",
       " ('1595', 1),\n",
       " ('19901993', 1),\n",
       " ('1997', 1),\n",
       " ('1a', 1),\n",
       " ('1b', 1),\n",
       " ('1c', 2),\n",
       " ('1d', 1),\n",
       " ('1e', 2),\n",
       " ('1e1', 5),\n",
       " ('1e2', 1),\n",
       " ('1e3', 1),\n",
       " ('1e4', 1),\n",
       " ('1e5', 1),\n",
       " ('1e6', 1),\n",
       " ('1e7', 3),\n",
       " ('1e8', 4),\n",
       " ('1e9', 3),\n",
       " ('1f', 1),\n",
       " ('1f1', 1),\n",
       " ('1f2', 1),\n",
       " ('1f3', 5),\n",
       " ('1f4', 1),\n",
       " ('1f5', 1),\n",
       " ('1f6', 1),\n",
       " ('2', 10),\n",
       " ('20', 1),\n",
       " ('2001', 1),\n",
       " ('2012', 1),\n",
       " ('25', 1),\n",
       " ('3', 6),\n",
       " ('30', 1),\n",
       " ('4', 3),\n",
       " ('4557', 1),\n",
       " ('5', 1)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_count_rbk=data_count.reduceByKey(lambda x,y:(x+y)).sortByKey()\n",
    "data_count_rbk.take(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Clasifica las palabras</strong>\n",
    "\n",
    "Queremos ordenar las palabras más frecuentes en orden descendente. Como primer paso, cambiamos los pares (clave, val) como (val, clave)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(16, '1'),\n",
       " (1, '100'),\n",
       " (2, '1112'),\n",
       " (1, '1112txt'),\n",
       " (1, '1112zip'),\n",
       " (1, '1500'),\n",
       " (1, '1595'),\n",
       " (1, '19901993'),\n",
       " (1, '1997'),\n",
       " (1, '1a')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_count_rbk=data_count_rbk.map(lambda x:(x[1],x[0]))\n",
    "data_count_rbk.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(866, 'the'),\n",
       " (793, 'and'),\n",
       " (621, 'to'),\n",
       " (584, 'i'),\n",
       " (535, 'of'),\n",
       " (527, 'a'),\n",
       " (377, 'in'),\n",
       " (373, 'is'),\n",
       " (363, 'that'),\n",
       " (360, 'my')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_count_rbk.sortByKey(False).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Excluir StopWords</strong>\n",
    "\n",
    "Para excluir palabras vacías, descargamos la biblioteca nltk y obtenemos la lista de palabras vacías en inglés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando excluimos valores de palabras vacías, vemos que la palabra \"thou\" es la palabra más común."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(276, 'thou'),\n",
       " (164, 'thy'),\n",
       " (163, 'rom'),\n",
       " (147, 'nurse'),\n",
       " (134, 'thee'),\n",
       " (131, 'love'),\n",
       " (128, 'romeo'),\n",
       " (117, 'jul'),\n",
       " (112, 'shall'),\n",
       " (97, 'come'),\n",
       " (91, 'friar'),\n",
       " (90, 'project'),\n",
       " (83, 'ill'),\n",
       " (82, 'enter'),\n",
       " (82, 'good'),\n",
       " (74, 'go'),\n",
       " (70, 'well'),\n",
       " (68, 'man'),\n",
       " (66, 'death'),\n",
       " (66, 'may')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_count_rbk=data_count_rbk.filter(lambda x: x[1] not in stopwords).sortByKey(False)\n",
    "data_count_rbk.sortByKey(False).take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliografía\n",
    "<ul>\n",
    "<li><a href=\"https://www.docker.com\">https://www.docker.com</a></li>\n",
    "<li><a href=\"https://spark.apache.org\">https://spark.apache.org</a></li>\n",
    "    <li><a href=\"https://spark.apache.org/docs/latest/api/python/index.html\">https://spark.apache.org/docs/latest/api/python/index.html</a></li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
